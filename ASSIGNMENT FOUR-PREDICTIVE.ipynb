{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c2dac12a",
   "metadata": {},
   "source": [
    "KENNEDY BWIRE 21/02900"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c7efaec",
   "metadata": {},
   "source": [
    "About the Assignment\n",
    "The dataset was derived from kaggle.com. The premise of the dataset is to predict whether a patient who has been diagonised with stroke heart-related complications is likely to be hit by a stroke.The work will be anchored on the following;\n",
    "1. Loading the dataset and performing the preprocesing activities\n",
    "2. Creating Training and Validation set.\n",
    "3. Defining the Archictecture of the Model\n",
    "4. Training the Model\n",
    "5. Evaluating the model performance on the validation set\n",
    "6. Visualize the model. \n",
    "7. Conclusion. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "88a3a7f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: sklearn in c:\\users\\ken bwire\\anaconda3\\lib\\site-packages (0.0)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\ken bwire\\anaconda3\\lib\\site-packages (from sklearn) (1.1.3)\n",
      "Requirement already satisfied: numpy>=1.17.3 in c:\\users\\ken bwire\\anaconda3\\lib\\site-packages (from scikit-learn->sklearn) (1.21.5)\n",
      "Requirement already satisfied: scipy>=1.3.2 in c:\\users\\ken bwire\\anaconda3\\lib\\site-packages (from scikit-learn->sklearn) (1.7.3)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\ken bwire\\anaconda3\\lib\\site-packages (from scikit-learn->sklearn) (2.2.0)\n",
      "Requirement already satisfied: joblib>=1.0.0 in c:\\users\\ken bwire\\anaconda3\\lib\\site-packages (from scikit-learn->sklearn) (1.2.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install sklearn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c105868",
   "metadata": {},
   "source": [
    "# Loading the dataset and performing the preprocesing activities. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae514647",
   "metadata": {},
   "source": [
    "1. Selection\n",
    "Importing the necessary dependencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0c78eb75",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt \n",
    "%matplotlib inline\n",
    "from matplotlib import style\n",
    "style.use(\"ggplot\")\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import Dense,Embedding,LSTM, GRU, Bidirectional\n",
    "from keras.optimizers import SGD\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27fb7233",
   "metadata": {},
   "source": [
    "Loading the csv data to a panda dataframe for data inspection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "25303436",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'C:\\\\pydatafiles\\\\healthcare-dataset-stroke-data.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[1;32mIn [5]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m stroke\u001b[38;5;241m=\u001b[39m\u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mC:\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43mpydatafiles\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43mhealthcare-dataset-stroke-data.csv\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(stroke)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\util\\_decorators.py:311\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    305\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) \u001b[38;5;241m>\u001b[39m num_allow_args:\n\u001b[0;32m    306\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m    307\u001b[0m         msg\u001b[38;5;241m.\u001b[39mformat(arguments\u001b[38;5;241m=\u001b[39marguments),\n\u001b[0;32m    308\u001b[0m         \u001b[38;5;167;01mFutureWarning\u001b[39;00m,\n\u001b[0;32m    309\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39mstacklevel,\n\u001b[0;32m    310\u001b[0m     )\n\u001b[1;32m--> 311\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:680\u001b[0m, in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[0;32m    665\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[0;32m    666\u001b[0m     dialect,\n\u001b[0;32m    667\u001b[0m     delimiter,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    676\u001b[0m     defaults\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdelimiter\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m\"\u001b[39m},\n\u001b[0;32m    677\u001b[0m )\n\u001b[0;32m    678\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[1;32m--> 680\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:575\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    572\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[0;32m    574\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[1;32m--> 575\u001b[0m parser \u001b[38;5;241m=\u001b[39m TextFileReader(filepath_or_buffer, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m    577\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[0;32m    578\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:933\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m    930\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m    932\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m--> 933\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:1217\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[1;34m(self, f, engine)\u001b[0m\n\u001b[0;32m   1213\u001b[0m     mode \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1214\u001b[0m \u001b[38;5;66;03m# error: No overload variant of \"get_handle\" matches argument types\u001b[39;00m\n\u001b[0;32m   1215\u001b[0m \u001b[38;5;66;03m# \"Union[str, PathLike[str], ReadCsvBuffer[bytes], ReadCsvBuffer[str]]\"\u001b[39;00m\n\u001b[0;32m   1216\u001b[0m \u001b[38;5;66;03m# , \"str\", \"bool\", \"Any\", \"Any\", \"Any\", \"Any\", \"Any\"\u001b[39;00m\n\u001b[1;32m-> 1217\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[call-overload]\u001b[39;49;00m\n\u001b[0;32m   1218\u001b[0m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1219\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1220\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1221\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcompression\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1222\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmemory_map\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1223\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1224\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding_errors\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstrict\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1225\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstorage_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1226\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1227\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1228\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\common.py:789\u001b[0m, in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    784\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m    785\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[0;32m    786\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[0;32m    787\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[0;32m    788\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[1;32m--> 789\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[0;32m    790\u001b[0m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    791\u001b[0m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    792\u001b[0m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    793\u001b[0m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    794\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    795\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    796\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    797\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[0;32m    798\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'C:\\\\pydatafiles\\\\healthcare-dataset-stroke-data.csv'"
     ]
    }
   ],
   "source": [
    "stroke=pd.read_csv(\"C:\\pydatafiles\\healthcare-dataset-stroke-data.csv\")\n",
    "print(stroke)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a2ed331",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Understanding the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e77e104f",
   "metadata": {},
   "source": [
    "Printing the first 5 rows of the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f20ef70",
   "metadata": {},
   "outputs": [],
   "source": [
    "stroke.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ab13e3b",
   "metadata": {},
   "source": [
    "Getting some info about the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b60a0f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "stroke.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b25dbdd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#statistical measures about the dataset\n",
    "stroke.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6af46c7",
   "metadata": {},
   "source": [
    "2. Data cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba276dc2",
   "metadata": {},
   "source": [
    "#Checking for missing values in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43377865",
   "metadata": {},
   "outputs": [],
   "source": [
    "stroke.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6133fac1",
   "metadata": {},
   "source": [
    "Filling missing values with the mean. Float data type are denoted by the numerical variable. The data is filled with the mean value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ea3b330",
   "metadata": {},
   "outputs": [],
   "source": [
    "stroke['bmi'].fillna(stroke['bmi'].mean(),inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bd7e497",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Checking the dataset\n",
    "stroke.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c2ebcad",
   "metadata": {},
   "source": [
    "#converting categories to Numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5c667ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Converting the categories into numbers using map function.\n",
    "stroke['gender']=stroke['gender'].map({'Male':0,'Female':1})\n",
    "stroke['ever_married']=stroke['ever_married'].map({'Yes':0,'No':1})\n",
    "stroke['work_type']=stroke['work_type'].map({'Private':0,'Self-employed':1})\n",
    "stroke['Residence_type']=stroke['Residence_type'].map({'Rural':0,'Urban':1})\n",
    "stroke['smoking_status']=stroke['smoking_status'].map({'smokes':0,'formerly smoked':1,'never smoked':2})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a75f6611",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Bringing all the variables in range of 0 and 1\n",
    "stroke['gender']=stroke['gender']-stroke['gender'].min()/stroke['gender'].max()-stroke['gender'].min()\n",
    "stroke['age']=stroke['age']-stroke['age'].min()/stroke['age'].max()-stroke['age'].min()\n",
    "stroke['hypertension']=stroke['hypertension']-stroke['hypertension'].min()/stroke['hypertension'].max()-stroke['hypertension'].min()\n",
    "stroke['heart_disease']=stroke['heart_disease']-stroke['heart_disease'].min()/stroke['heart_disease'].max()-stroke['heart_disease'].min()\n",
    "stroke['ever_married']=stroke['ever_married']-stroke['ever_married'].min()/stroke['ever_married'].max()-stroke['ever_married'].min()\n",
    "stroke['work_type']=stroke['work_type']-stroke['work_type'].min()/stroke['work_type'].max()-stroke['work_type'].min()\n",
    "stroke['Residence_type']=stroke['Residence_type']-stroke['Residence_type'].min()/stroke['Residence_type'].max()-stroke['Residence_type'].min()\n",
    "stroke['avg_glucose_level']=stroke['avg_glucose_level']-stroke['avg_glucose_level'].min()/stroke['avg_glucose_level'].max()-stroke['avg_glucose_level'].min()\n",
    "stroke['bmi']=stroke['bmi']-stroke['bmi'].min()/stroke['bmi'].max()-stroke['bmi'].min()\n",
    "stroke['smoking_status']=stroke['smoking_status']-stroke['smoking_status'].min()/stroke['smoking_status'].max()-stroke['smoking_status'].min()\n",
    "stroke['stroke']=stroke['stroke']-stroke['stroke'].min()/stroke['stroke'].max()-stroke['stroke'].min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9afea09d",
   "metadata": {},
   "outputs": [],
   "source": [
    "stroke.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db947e22",
   "metadata": {},
   "source": [
    "Printng the numbers of rows and columns in the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6648db38",
   "metadata": {},
   "outputs": [],
   "source": [
    "stroke.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b21842b6",
   "metadata": {},
   "source": [
    "Checking the distribution of the target variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ecf244b",
   "metadata": {},
   "outputs": [],
   "source": [
    "stroke['stroke'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7356e510",
   "metadata": {},
   "source": [
    "Data Visualization. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7811fbec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# seaborn has an easy method to showcase heatmap\n",
    "plt.figure(figsize=(20,10))\n",
    "p = sns.heatmap(stroke.corr(), annot=True, cmap='RdYlGn')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a1867e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reducing the number of Rows\n",
    "stroke.drop(stroke.index[4000:538674], inplace=True)\n",
    "stroke.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f16eb240",
   "metadata": {},
   "outputs": [],
   "source": [
    "p= stroke.hist(figsize=(20,10),color='green')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dd7d8d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.scatterplot(x='age', y='gender', data=stroke)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "424d3e6a",
   "metadata": {},
   "source": [
    "3. Data Transformation\n",
    "This refers to modifying data so that can be ready for predictive analytics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63342bca",
   "metadata": {},
   "source": [
    "Transform Target variables to Numeric array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cba5856f",
   "metadata": {},
   "outputs": [],
   "source": [
    "Encoder=preprocessing.LabelEncoder()\n",
    "Encoded_stroke=stroke.apply(preprocessing.LabelEncoder().fit_transform)\n",
    "print(\"Transformed Data:\\n\",Encoded_stroke)\n",
    "Numeric_Array=Encoded_stroke.values\n",
    "print(\"Numeric Array\\n\",Numeric_Array)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ec16ccd",
   "metadata": {},
   "source": [
    "# Create the Training and Validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c2bded6",
   "metadata": {},
   "outputs": [],
   "source": [
    "Training_Sample,Test_Sample=train_test_split(Numeric_Array,test_size=0.2,random_state=2)\n",
    "print(\"Training Sample:\\n\",Training_Sample)\n",
    "print(\"Test Sample:\\n\",Test_Sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d59da637",
   "metadata": {},
   "source": [
    "Select Input and output variable from training sample and test sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "020dc129",
   "metadata": {},
   "outputs": [],
   "source": [
    "XTrain_Sample=Training_Sample[:,1]\n",
    "print(\"Input Attributes of Training Sample\\n\",XTrain_Sample)\n",
    "YTrain_Sample=Training_Sample[:,-1]\n",
    "print(\"Output Attributes of Training Sample\\n\\n\",YTrain_Sample)\n",
    "XTest_Sample=Test_Sample[:,:-1]\n",
    "print(\"Input Attributes of Test Sample\\n\",XTest_Sample)\n",
    "Actual_YTest_Sample=Test_Sample[:,-1]\n",
    "print(\"Actual Test Sample Classes\\n\\n\",Actual_YTest_Sample)\n",
    "XTrain_Sample.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c428a5ab",
   "metadata": {},
   "source": [
    "In using the PCA the input attributes of both Training and Test samples sre compressed into two attributes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9b7012a",
   "metadata": {},
   "outputs": [],
   "source": [
    "pca=PCA(n_components=2)\n",
    "XTrain_Sample=Training_Sample[:,0:-1]\n",
    "pca.fit(XTrain_Sample)\n",
    "Decomposed_XTrain_Sample=pca.transform(XTrain_Sample)\n",
    "print(\"\\nDecomposed Input Attributes\\n\",Decomposed_XTrain_Sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11941316",
   "metadata": {},
   "source": [
    "# Defining the Architecture of the Model\n",
    "This involves creating the model and specidy the number of the input neutrons, defining the input neutrons,specify the no of output neutrons and the number of hidden layers and the hidden neutrons. For instance the XTrain_Sample.shape[1] defines the input neutrons, and since the data is binary classification, we have a single neutron in the output layer hence the output neutron is 1. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a97abebe",
   "metadata": {},
   "outputs": [],
   "source": [
    "#The LSTM Architecture\n",
    "regressor= Sequential()\n",
    "#First LSTM layer with Dropout regularisation\n",
    "regressor.add(LSTM(units=50, return_sequences=True, input_shape=(XTrain_Sample.shape[1],1)))\n",
    "regressor.add(Dropout(0.2))\n",
    "# Second LSTM layer\n",
    "regressor.add(LSTM(units=50, return_sequences=True))\n",
    "regressor.add(Dropout(0.2))\n",
    "# Third LSTM layer\n",
    "regressor.add(LSTM(units=50, return_sequences=True))\n",
    "regressor.add(Dropout(0.2))\n",
    "# Fourth LSTM layer\n",
    "regressor.add(LSTM(units=50))\n",
    "regressor.add(Dropout(0.2))\n",
    "# The output layer\n",
    "regressor.add(Dense(units=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2a2a3a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "regressor.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc7113f3",
   "metadata": {},
   "source": [
    "Compiling the RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c6f8719",
   "metadata": {},
   "outputs": [],
   "source": [
    "regressor.compile(optimizer='rmsprop',loss='mean_squared_error',metrics='accuracy')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f935478",
   "metadata": {},
   "source": [
    "# Training the Model\n",
    "This involve passing the independent and dependent variable features for the training set for training the model. Validation data will be evaluated at the end of each epoch. The espoch is set at 50. The trained model in the model history variable will be used to visualize the training process. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66147972",
   "metadata": {},
   "source": [
    "Fitting to the training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b348c69",
   "metadata": {},
   "outputs": [],
   "source": [
    "regressor.fit(XTrain_Sample,YTrain_Sample,epochs=50,batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22f98ba1",
   "metadata": {},
   "outputs": [],
   "source": [
    "regressor.history=regressor.fit(XTrain_Sample,YTrain_Sample,validation_data=(XTest_Sample,Actual_YTest_Sample),epochs=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5aa5be23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The GRU architecture\n",
    "regressorGRU = Sequential()\n",
    "# First GRU layer with Dropout regularisation\n",
    "regressorGRU.add(GRU(units=50, return_sequences=True, input_shape=(XTrain_Sample.shape[1],1), activation='tanh'))\n",
    "regressorGRU.add(Dropout(0.2))\n",
    "# Second GRU layer\n",
    "regressorGRU.add(GRU(units=50, return_sequences=True, input_shape=(XTrain_Sample.shape[1],1), activation='tanh'))\n",
    "regressorGRU.add(Dropout(0.2))\n",
    "# Third GRU layer\n",
    "regressorGRU.add(GRU(units=50, return_sequences=True, input_shape=(XTrain_Sample.shape[1],1), activation='tanh'))\n",
    "regressorGRU.add(Dropout(0.2))\n",
    "# Fourth GRU layer\n",
    "regressorGRU.add(GRU(units=50, activation='tanh'))\n",
    "regressorGRU.add(Dropout(0.2))\n",
    "# The output layer\n",
    "regressorGRU.add(Dense(units=1))\n",
    "# Compiling the RNN\n",
    "regressorGRU.compile(optimizer=SGD(lr=0.01, decay=1e-7, momentum=0.9, nesterov=False),loss='mean_squared_error',metrics='Accuracy')\n",
    "# Fitting to the training set\n",
    "regressorGRU.fit(XTrain_Sample,YTrain_Sample,epochs=50,batch_size=150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "327b4b87",
   "metadata": {},
   "outputs": [],
   "source": [
    "regressor.history=regressor.fit(XTrain_Sample,YTrain_Sample,validation_data=(XTest_Sample,Actual_YTest_Sample),epochs=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dac320d",
   "metadata": {},
   "source": [
    "# Evaluation of the model Performance on Validation set."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc0bb732",
   "metadata": {},
   "source": [
    "The validation set had a accuracy score of 94.06% and a validation accuracy of 92.25%.This signifies that the model was trained well on the training data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa83d3e0",
   "metadata": {},
   "source": [
    "# Visualizing the model performance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eec68f17",
   "metadata": {},
   "source": [
    "Loss refers to the loss value over the training data after each epoch. This is what the optimization process is trying to minimize with the training so, the lower, the better.Accuracy refers to the ratio between correct predictions and the total number of predictions in the training data. The higher, the better. This is normally inversely correlated with the loss, but not always"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d783e1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Summarize history for Loss. \n",
    "plt.figure(figsize=(15,10))\n",
    "plt.plot(regressor.history.history['loss'])\n",
    "plt.plot(regressor.history.history['val_loss'])\n",
    "plt.title(\"A GRAPH SHOWING LOSS AGAINST EPOCH\")\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend(['Train Accuracy','Validation Accuracy'],loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e0fd9ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Summarize the history for Accuracy\n",
    "plt.figure(figsize=(15,10))\n",
    "plt.plot(regressor.history.history['accuracy'])\n",
    "plt.plot(regressor.history.history['val_accuracy'])\n",
    "plt.title(\"A MODEL OF LOSS VS EPOCH\")\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend(['Train Accuracy','Validation Accuracy'],loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f980a97d",
   "metadata": {},
   "source": [
    "# Conclusion. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e300c83a",
   "metadata": {},
   "source": [
    "From the data,the accuracy score of 94.06% and a validation accuracy of 92.25% signifies a perfect relationship between the independent variable and dependent variable. The positive result on validation data means that the model can be utilised to predict and confirm the likelihood of a patient diagonosed with heart related disease might or might not suffer from the stroke. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "918ed14e",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install pycaret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b253517f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "import pycaret\n",
    "from pycaret.classification import *\n",
    "import pandas as pd\n",
    "import category_encoders as ce\n",
    "stroke=pd.read_csv(\"C:\\pydatafiles\\healthcare-dataset-stroke-data.csv\")\n",
    "\n",
    "encoder= ce.OrdinalEncoder(cols=['gender'],return_df=True, mapping=[{'col':'gender', 'mapping':{0: 1, 1: 2,'Other': 3}}])\n",
    "stroke['gender'] = encoder.fit_transform(stroke['gender'])\n",
    "encoder= ce.OrdinalEncoder(cols=['work_type'],return_df=True, mapping=[{'col':'work_type', 'mapping':{0: 1, 1: 2, 'children': 3, '2': 4, 'Never_worked': 5}}])\n",
    "stroke['work_type'] = encoder.fit_transform(stroke['work_type'])\n",
    "\n",
    "s = setup(data =stroke, target = 'stroke', fix_imbalance = True, session_id=123)\n",
    "\n",
    "best = compare_models()\n",
    "compare_model_results = pull()\n",
    "            \n",
    "model = gr.inputs.Dropdown(list(compare_model_results['Model']),label=\"Model\")\n",
    "gender = gr.inputs.Dropdown(choices=[\"Male\", \"Female\"],label = 'gender')\n",
    "age = gr.inputs.Slider(minimum=1, maximum=100, default=data['age'].mean(), label = 'age')\n",
    "hypertension = gr.inputs.Dropdown(choices=[\"1\", \"0\"],label = 'hypertension')\n",
    "heart_disease = gr.inputs.Dropdown(choices=[\"1\", \"0\"],label ='heart_disease')\n",
    "ever_married = gr.inputs.Dropdown(choices=[\"Yes\", \"No\"], label ='ever_married')\n",
    "work_type = gr.inputs.Dropdown(choices=[\"children\", \"Govt_job\",\"Never_worked\",\"Private\",\"Self-employed\"],label = 'work_type')\n",
    "Residence_type = gr.inputs.Dropdown(choices=[\"Urban\", \"Rural\"],label = 'Residence_type')\n",
    "avg_glucose_level =\tgr.inputs.Slider(minimum=-55, maximum=300, default=data['avg_glucose_level'].mean(), label = 'avg_glucose_level')\n",
    "bmi = gr.inputs.Slider(minimum=-10, maximum=100, default=data['bmi'].mean(), label = 'bmi')\n",
    "smoking_status = gr.inputs.Dropdown(choices=[\"Unknown\", \"smokes\",\"never_smoked\", \"formerly_smoked\"], label ='smoking_status')\n",
    "\n",
    "gr.Interface(predict,[model, gender, age, hypertension, heart_disease, ever_married, work_type, Residence_type, avg_glucose_level, bmi, smoking_status], \"label\",live=True).launch()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ced23881",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
